{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b05bbc0-4b35-4d47-8f89-5befab0aac66",
   "metadata": {},
   "source": [
    "# Excerpt of Machine Learning Project — Università Bocconi (Spring 2025)\n",
    "\n",
    "This notebook presents selected excerpts of the code I developed for my Machine Learning coursework project at Università Bocconi.\n",
    "\n",
    "**Note:** The dataset used in the original project is not included here for confidentiality reasons.  \n",
    "The purpose of this notebook is to demonstrate the main preprocessing and modeling pipelines (Ridge and Random Forest regressions) I implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75e8d5-4856-4abb-b099-dc041052f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Model construction and evaluation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ensembles\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93936c-00e6-4341-a153-9c81d13f05a4",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Below are the preprocessing steps applied to clean and prepare the dataset prior to model training.\n",
    "These included handling missing values, encoding categorical features, and log-transforming the target variable.\n",
    "Separate preprocessing was performed for outfielders and goalkeepers to optimize model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fc9b2-7a62-4a8a-8c3f-9b9acecde572",
   "metadata": {},
   "source": [
    "*Log-transform target 'value_eur'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505adeab-6294-4092-88e4-741c50fc6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new dataset\n",
    "df_train_log = df_train_raw.copy()\n",
    "\n",
    "# Apply logarithmic transformation\n",
    "df_train_log['log_value_eur'] = np.log1p(df_train_log['value_eur'])\n",
    "\n",
    "# Drop the original 'value_eur' column\n",
    "df_train_log = df_train_log.drop(columns=['value_eur'])\n",
    "\n",
    "# Drop columns with >30% missing\n",
    "# Drop missing value_eur rows\n",
    "df_train_clean = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbd1e8-f0c0-402b-b3ba-3aedeb3d92c1",
   "metadata": {},
   "source": [
    "*One-Hot Encode Dummies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47feff-4885-47c0-ae95-a50ed7643ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = pd.get_dummies(df_train_clean, columns=['preferred_foot','work_rate','body_type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997cec7-4365-4174-b14f-14ff2acd6be6",
   "metadata": {},
   "source": [
    "*Impute missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e8db2-68eb-481a-b5d3-1dee9faaa602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns excluding subset-specific columns\n",
    "numeric_cols = df_train_clean.select_dtypes(include=['number']).columns\n",
    "excluded_cols = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic', 'goalkeeping_speed']\n",
    "cols_to_impute = [col for col in numeric_cols if col not in excluded_cols]\n",
    "\n",
    "# Initialize the median imputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Apply imputation only to the selected numeric columns\n",
    "df_train_clean[cols_to_impute] = imputer.fit_transform(df_train_clean[cols_to_impute])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae961-9695-4897-93fb-190e97c83cd7",
   "metadata": {},
   "source": [
    "*Partition into 'Goalkeepers' and 'Outfielders'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5967d05-0007-4d51-a27b-c4420f30f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define unique features\n",
    "gk_features = ['goalkeeping_speed']\n",
    "of_features = ['pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic']\n",
    "\n",
    "# Step 2: Identify shared features\n",
    "exclude_features = gk_features + of_features + ['player_positions']\n",
    "shared_features = [col for col in df_train_clean.columns if col not in exclude_features]\n",
    "\n",
    "# Step 3: Define masks\n",
    "gk_mask = df_train_clean['player_positions'].str.contains('GK', na=False)\n",
    "of_mask = ~gk_mask\n",
    "\n",
    "# Step 4: Build feature sets\n",
    "gk_full_features = shared_features + gk_features\n",
    "of_full_features = shared_features + of_features\n",
    "\n",
    "# Step 5: Partition the data\n",
    "df_train_gk = df_train_clean.loc[gk_mask, list(set(gk_full_features + ['log_value_eur']))].copy()\n",
    "df_train_of = df_train_clean.loc[of_mask, list(set(of_full_features + ['log_value_eur']))].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc2dc5-4589-4071-8088-2375e34baba4",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab61be6-b824-49c8-b360-8eb6a80b2197",
   "metadata": {},
   "source": [
    "To address multicollinearity identified in the feature space, Ridge regression was implemented using a standardized pipeline for both subsets of players — **Goalkeepers** and **Outfielders**. Each subset was modeled independently to account for position-specific features that were mutually exclusive between groups. The pipeline applied z-score standardization (`StandardScaler`) before training the Ridge estimator. \n",
    "\n",
    "A **GridSearchCV** procedure with 5-fold cross-validation was used to tune the regularization parameter `alpha` over 100 equally spaced values between 0.05 and 5.0. The model was evaluated using the coefficient of determination (R²) as the scoring metric, ensuring that the selected hyperparameter generalized well across folds. After identifying the optimal `alpha` separately for each subset, final models were retrained on the full training data, and **in-sample RMSE** was computed on the log-transformed target variable. To obtain interpretable estimates of model error, RMSE values were later rescaled back to the normal (euro) scale using the mean of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a324fe3-efc3-4516-86ed-bc0058ffc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and targets\n",
    "X_gk = df_train_gk.drop(columns=['log_value_eur'])\n",
    "y_gk = df_train_gk['log_value_eur']\n",
    "\n",
    "X_of = df_train_of.drop(columns=['log_value_eur'])\n",
    "y_of = df_train_of['log_value_eur']\n",
    "\n",
    "# Create dictionary datasets\n",
    "datasets = {\n",
    "    'Goalkeepers': (X_gk, y_gk),\n",
    "    'Outfielders': (X_of, y_of)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a1a9b-bb6d-42cf-b184-ac8acc9d35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ridge pipeline\n",
    "ridge_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_ridge = {\n",
    "    'ridge__alpha': np.linspace(0.05, 5.0, 100)\n",
    "}\n",
    "\n",
    "# Create dictionary of datasets\n",
    "datasets = {\n",
    "    'Goalkeepers': (X_gk, y_gk),\n",
    "    'Outfielders': (X_of, y_of)\n",
    "}\n",
    "\n",
    "# Run GridSearchCV for each dataset\n",
    "results = {}\n",
    "\n",
    "for name, (X, y) in datasets.items():\n",
    "    ridge_grid = GridSearchCV(\n",
    "        estimator=ridge_pipe,\n",
    "        param_grid=param_grid_ridge,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    ridge_grid.fit(X, y)\n",
    "    results[name] = {\n",
    "        'best_alpha_ridge': ridge_grid.best_params_['ridge__alpha'],\n",
    "        'best_score_ridge': ridge_grid.best_score_\n",
    "    }\n",
    "    print(f\"{name} — Best alpha: {results[name]['best_alpha_ridge']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43f2bb-7f71-4991-87e0-08dbca22e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge model evaluation\n",
    "ridge_results = {}\n",
    "\n",
    "for name, (X, y) in datasets.items():\n",
    "    best_alpha_ridge = results[name]['best_alpha_ridge']\n",
    "    \n",
    "    # Create final pipeline with best alpha\n",
    "    final_pipe_ridge = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge(alpha=best_alpha_ridge))\n",
    "    ])\n",
    "    \n",
    "    # Fit model on full training data\n",
    "    final_pipe_ridge.fit(X, y)\n",
    "    \n",
    "    # Predict on training data\n",
    "    preds = final_pipe_ridge.predict(X)\n",
    "    \n",
    "    # Compute in-sample RMSE\n",
    "    rmse_in_sample = np.sqrt(mean_squared_error(y, preds))\n",
    "\n",
    "    ridge_results[name] = {\n",
    "        'best_alpha_ridge': best_alpha_ridge,\n",
    "        'rmse_in_sample': rmse_in_sample,\n",
    "    }\n",
    "\n",
    "    print(f\"{name} — Best alpha: {best_alpha_ridge}, In-sample RMSE: {rmse_in_sample:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d7f89-7f6a-440a-a084-142e60b572dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE values (use normal-scale values here)\n",
    "ridge_rmse_gk = X\n",
    "ridge_rmse_of = Y\n",
    "\n",
    "# Combined RMSE\n",
    "ridge_rmse_c = np.sqrt((n_gk * ridge_rmse_gk**2 + n_of * ridge_rmse_of**2) / (n_gk + n_of))\n",
    "\n",
    "# Convert log-scale RMSE to normal-scale approximation\n",
    "ridge_rmse = mean_value * (np.exp(ridge_rmse_c) - 1)\n",
    "\n",
    "print(f\"RIDGE RMSE: €{ridge_rmse:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb067a-c69a-48d8-9065-3e58ddc842c3",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831be60-bc76-4fb4-bce1-0a33ec32c59f",
   "metadata": {},
   "source": [
    "To capture nonlinear relationships and high-order feature interactions beyond the capacity of linear models, separate **Random Forest Regressors** were trained for the **Goalkeeper** and **Outfielder** subsets. Each model used bootstrapped sampling and the out-of-bag (OOB) estimation procedure to evaluate generalization error without requiring an explicit validation split, thus maximizing data usage. \n",
    "\n",
    "The pipeline employed feature subsampling (`max_features='sqrt'`) and tree averaging (`n_estimators=100`) to reduce variance and mitigate overfitting. OOB predictions provided an unbiased estimate of RMSE for each subset, while combined OOB performance was computed by concatenating predictions from both models. The Random Forest approach demonstrated superior predictive accuracy and stability, achieving the lowest RMSE and highest R² (up to **0.996**) among all tested algorithms. Model errors were subsequently transformed from log to euro scale for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d10f66-3f54-4961-9a30-0a5a64090251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goalkeepers\n",
    "gk_features = df_train_gk.drop(columns=['log_value_eur'])\n",
    "gk_target   = df_train_gk['log_value_eur']\n",
    "\n",
    "# Outfielders\n",
    "of_features = df_train_of.drop(columns=['log_value_eur'])\n",
    "of_target   = df_train_of['log_value_eur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45cb53-81da-4d98-8f8a-c46df59a2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline with StandardScaler and RF (OOB enabled)\n",
    "def build_rf_pipeline_oob(n_estimators=100, max_depth=None, max_features='sqrt'):\n",
    "    return Pipeline([\n",
    "        ('rfoob', RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            max_features=max_features,\n",
    "            bootstrap=True,\n",
    "            oob_score=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# Train for Goalkeepers\n",
    "print(\"Training Random Forest with OOB for Goalkeepers...\")\n",
    "rf_gk_pipeline = build_rf_pipeline_oob()\n",
    "rf_gk_pipeline.fit(gk_features, gk_target)\n",
    "rf_gk_model = rf_gk_pipeline.named_steps['rfoob']\n",
    "gk_oob_rmse = mean_squared_error(gk_target, rf_gk_model.oob_prediction_, squared=False)\n",
    "print(f\"Goalkeepers OOB RMSE: {gk_oob_rmse:.4f}\")\n",
    "\n",
    "# Train for Outfielders\n",
    "print(\"\\nTraining Random Forest with OOB for Outfielders...\")\n",
    "rf_of_pipeline = build_rf_pipeline_oob()\n",
    "rf_of_pipeline.fit(of_features, of_target)\n",
    "rf_of_model = rf_of_pipeline.named_steps['rfoob']\n",
    "of_oob_rmse = mean_squared_error(of_target, rf_of_model.oob_prediction_, squared=False)\n",
    "print(f\"Outfielders OOB RMSE: {of_oob_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad34ed7e-92f9-4f6d-b4c6-06b9d83b4cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions and truths\n",
    "rf_oob_preds = np.concatenate([rf_gk_model.oob_prediction_, rf_of_model.oob_prediction_])\n",
    "rf_oob_truth = np.concatenate([gk_target, of_target])\n",
    "\n",
    "overall_rf_oob_rmse = mean_squared_error(rf_oob_truth, rf_oob_preds, squared=False)\n",
    "print(f\"\\nCombined Random Forest OOB RMSE: {overall_rf_oob_rmse:.4f}\")\n",
    "\n",
    "rmse_rf_oob_normal = mean_value * (np.exp(overall_rf_oob_rmse) - 1)\n",
    "\n",
    "print(f\"Random Forest RMSE: €{rmse_rf_oob_normal:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
